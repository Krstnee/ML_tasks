# -*- coding: utf-8 -*-
"""lab4_Епифанова.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wCmOuwTv7fOtqPHKUtQRJIY-Ov6qnf5q
"""

import nltk

nltk.download('punkt')

import requests
from nltk import sent_tokenize

url = 'https://s3.amazonaws.com/text-datasets/nietzsche.txt'
response = requests.get(url)
text = response.text

corpus_length = len(text)
print(f"А) Длина всего корпуса: {corpus_length} символов")

sentences = sent_tokenize(text)
num_sentences = len(sentences)
print(f"Б) Количество предложений: {num_sentences} предложений")

num_characters = sum(len(sentence) for sentence in sentences)
print(f"В) Количество символов: {num_characters} символов")

import requests
import numpy as np
from nltk import sent_tokenize
chars = sorted(list(set(text)))

char_indices = dict((char, i) for i, char in enumerate(chars))

maxlen = 20
step = 3
sentences = []
next_chars = []

for i in range(0, len(text) - maxlen, step):
    sentences.append(text[i: i + maxlen])
    next_chars.append(text[i + maxlen])

print('nb sequences:', len(sentences))

print('Vectorization...')
x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)

for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        x[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import LambdaCallback
import random
import sys

model = Sequential()
model.add(LSTM(128, input_shape=(maxlen, len(chars))))
model.add(Dense(len(chars), activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam')

def sample(preds, temperature=1.0):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)

def on_epoch_end(epoch, _):
    print('\n----- Generating text after Epoch: %d' % epoch)

    start_index = random.randint(0, len(text) - maxlen - 1)
    generated_text = text[start_index: start_index + maxlen]

    print('----- Generating with seed: "' + generated_text + '"')

    for temperature in [0.2, 0.5, 1.0, 1.2]:
        print('----- Temperature:', temperature)
        sys.stdout.write(generated_text)

        for i in range(400):
            sampled = np.zeros((1, maxlen, len(chars)))
            for t, char in enumerate(generated_text):
                sampled[0, t, char_indices[char]] = 1.

            preds = model.predict(sampled, verbose=0)[0]
            next_index = sample(preds, temperature)
            next_char = chars[next_index]

            generated_text += next_char
            generated_text = generated_text[1:]

            sys.stdout.write(next_char)
            sys.stdout.flush()
        print()
print_callback = LambdaCallback(on_epoch_end=on_epoch_end)

model.fit(x, y, batch_size=128, epochs=20, callbacks=[print_callback])

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import LambdaCallback
import random
import sys
import sqlite3

conn = sqlite3.connect('./drive/My Drive/databases/wikibooks.sqlite')
cursor = conn.cursor()

cursor.execute("SELECT body_text FROM ru")
text_tuples = cursor.fetchall()[0:100][:]

conn.close()

text

import numpy as np
import string
import re


text1 = ' '.join([t[0] for t in text_tuples])
text = re.sub(r'[^А-Яа-яЁё\s\.,!?]', '', text1)

maxlen = 40
step = 3

sentences = []
next_chars = []

for i in range(0, len(text) - maxlen, step):
    sentences.append(text[i: i + maxlen])
    next_chars.append(text[i + maxlen])

allowed_characters = set(''.join([string.ascii_lowercase, string.ascii_uppercase, 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя', 'ЙЦУКЕНГШЩЗХЪФЫВАПРОЛДЖЭЯЧСМИТЬБЮЁ']) + string.punctuation + ' ' +  '\n' + '\t' + '\r' + '%' + '\xa0')
chars = sorted(list(set(filter(lambda char: char in allowed_characters, text))))

char_indices = {char: i for i, char in enumerate(chars)}
x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)

for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        if char in allowed_characters:
            x[i, t, char_indices[char]] = 1
    if next_chars[i] in allowed_characters:
        y[i, char_indices[next_chars[i]]] = 1

print(x[1:10], y[1:10])
print(char_indices)
print(text)

import torch
from transformers import BertTokenizer, BertForSequenceClassification

# Load pre-trained BERT model and tokenizer
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Example customer reviews
reviews = [
    "This product is amazing! I love it.",
    "The customer service was terrible. I would not recommend this company.",
    "I'm satisfied with my purchase. The quality is great.",
]

# Tokenize and encode the reviews
encoded_inputs = tokenizer(reviews, padding=True, truncation=True, return_tensors='pt')

# Make predictions
with torch.no_grad():
    outputs = model(**encoded_inputs)

# Get predicted labels
predicted_labels = torch.argmax(outputs.logits, dim=1)

# Map labels to sentiment categories
sentiment_labels = ['Positive', 'Negative']
sentiments = [sentiment_labels[label] for label in predicted_labels]

# Print the predicted sentiments for each review
for review, sentiment in zip(reviews, sentiments):
    print(f"Review: {review}")
    print(f"Sentiment: {sentiment}")
    print()



model = Sequential()
model.add(LSTM(128, input_shape=(maxlen, len(chars))))
model.add(Dense(len(chars), activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam')

def sample(preds, temperature=1.0):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)

def on_epoch_end(epoch, _):
    print('\n----- Generating text after Epoch: %d' % epoch)

    start_index = random.randint(0, len(text) - maxlen - 1)
    generated_text = text[start_index: start_index + maxlen]

    print('----- Generating with seed: "' + generated_text + '"')

    for temperature in [0.2, 0.5, 1.0, 1.2]:
        print('----- Temperature:', temperature)
        sys.stdout.write(generated_text)

        for i in range(400):
            sampled = np.zeros((1, maxlen, len(chars)))
            for t, char in enumerate(generated_text):
                sampled[0, t, char_indices[char]] = 1.

            preds = model.predict(sampled, verbose=0)[0]
            next_index = sample(preds, temperature)
            next_char = chars[next_index]

            generated_text += next_char
            generated_text = generated_text[1:]

            sys.stdout.write(next_char)
            sys.stdout.flush()
        print()

print_callback = LambdaCallback(on_epoch_end=on_epoch_end)

model.fit(x, y, batch_size=128, epochs=20, callbacks=[print_callback])